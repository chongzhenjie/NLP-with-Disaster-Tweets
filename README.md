# NLP with Disaster Tweets

[[Source](https://www.kaggle.com/competitions/nlp-getting-started)] 

### [Project](https://www.kaggle.com/code/chongzhenjie/disaster-tweets-basic-network-embeddings-bert): Predict which Tweets are about real disasters and which ones are not.

* Preprocessed dataset of 10,000 tweets and learned our own word embeddings using neural networks in TensorFlow Keras for text classification.
* Compared performance against BERT, a pre-trained language model which generates word embeddings that are context-sensitive. Fine-tuning the BERT model achieved better results in terms of F1 score, showing the benefits of transfer learning.
